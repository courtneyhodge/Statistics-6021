---
title: "Stats Notebook - Post Exam"
author: "Courtney Hodge"
date: "2024-07-30"
output: html_document
---

7/30/24 Class

```{r}
insurance <- read.csv("C:\\Users\\hodge\\Desktop\\UVA_Coding_Folder\\Statistics-6021\\insurance.csv")
```

```{r}
#view(insurance)
```

```{r}
modl <- lm(charges~smoker, data=insurance)

summary(modl)
```

```{r}
t.test(charges~smoker,data= insurance)
```

```{r}
new <- data.frame(smoker="yes")
predict(modl, new)
```

```{r}
mod2 <- lm(charges~region, data = insurance)
summary(mod2)
```
```{r}
insurance$region <-factor(insurance$region, levels = c("southeast", "northeast", "northwest", "southwest"))
```

```{r}
insurance$region2 <-factor(insurance$region, levels = c("southeast", "northeast", "northwest", "southwest"))

mod3 <- lm(charges~region2, data = insurance)
summary(mod3)
```

```{r}
anov <- aov(charges~region, data = insurance)
```


> why are we doing all of this work? For the ANOVA test in this case, it is telling us that at the 5% level, we can go ahead and reject the null, meaning at least one of these means are different. What after? Tukey comparison to see which one is really different.

```{r}
TukeyHSD(anov)
```
> At least we can see which of theintervals are both negative. Southwest tells us that on average, southwest is lover than southeast.

>Now, we are going back to smoker, but we are using age. Let's make a boxplot before we start to build the model. Here, age is our predictor.

```{r}
library(ggplot2)
ggplot(insurance, aes(x = age, y = charges)) + geom_jitter()
```
> IF we fit the model above, do you think it will be agood model? The answer is no.

```{r}
library(ggplot2)
ggplot(insurance, aes(x = age, y = charges)) + geom_jitter() + geom_smooth(method = "lm", se = F)
```

> we can see that the line is actually trying to do the middle of smoker charges instead of non-smoker charges.

> we are going to color the dots based on the smoker status. Now, we can see what's really going on and this is why **data exploration is important**. All the outliers, the high charges, on average, cooresponds to smokers.

```{r}
library(ggplot2)
ggplot(insurance, aes(x = age, y = charges, color = smoker)) + geom_point()
```

```{r}
library(ggplot2)
ggplot(insurance, aes(x = age, y = charges, color = smoker)) + geom_jitter() + geom_smooth(method = "lm", aes(group = smoker))
```
> now, we have two lines. one that makes a predictor for smokers and one that makes a predictor for non-smokers. Before we build the model, we will have an indicator function that fits the model.

```{r}
mod4<- lm(charges~age + smoker, data = insurance)
summary(mod4)
```

> in terms of an equation, what does this model look like? So, in terms of non-smokers, for every you that you grow, you pay $274.87 more dollars in your insurance plan. For smokeryes, every time you move from being a smoker to non smoker, your health insurance moves to 23855.30 at age 0.

## Using Dummy Variables to capture Interactions Between Explanatory Variables

```{r}
mod5 <- lm(charges~age * smoker, data = insurance)
summary(mod5)
```
> Every time your age incrases by 1, your smokers will increase by 267.25 + 37.99.

```{r}
ggplot(insurance, aes(x = age, y = charges, color = smoker)) + geom_jitter() + geom_smooth(method = "lm", model.extract(mod5), se = F)
```

```{r}
new_dat <- data.frame(age=26, smoker = "no")
predict(mod5, new_dat,interval = "prediction")
```

> for a particular person, Disha, her insurance cost will be within $-7700.646 $17414.75 thousands of dollars.

## building a model with everything
```{r}
ins <- insurance[,-8]
```

```{r}
mod10 <- lm(charges~., data = ins)
summary(mod10)
```
> the residual standard error meausres how well we are going to do. This model will error in health care coverage by about $6,062 dolalrs. SSE (sum of squared error) from the population model. MSE = SSE/(n-p-1) is the residual standard error. 
> SSE/n

## Outliers

> leverage points, influential points, outliers
> how do you use your model to get rid of some of these variables if they are presenting a big change to our model

> Outlier: Outliers are data points with the largest residuals. Detection:

1. Scatterplot
2. Residual Plot
3. Standardized Residuals: Residuals divided by their estimated standard deviation.

> An outlier can be an x-outlier (when predictor variable has an unusual value) or a y-outlier (when residual variable has an unusual value). 

>Treating Outliers: 
1. Transform response
2. Consider context and subject matter knowledge
3. Measurement error>
4. Winsorize: Replace extreme valeus with the nearest values that are not outliers. For ex., replace valeues about the 95th percentile with the value at the 95th percentile
5. Trim/revmove point (last resort)
6. Use other techniques like quantile regression or Random Forest which are resistant to outliers.

> Leverage Points: dat apoints that hvae **extreme values** for a predictor variable. These points have a large influence on the fitted values of the model, but not necessarily on the regression coefficients themselves.
>Detection:
1. Leverage values are in the diagonals of the Hat matrix:(X^TX)^-1(X^T)
2. High leverage points are identified by leverage values greater than 2(p+1)/n, wher p is the number of predictors and 

>Treating High Leverage Points
1. Transform predicor
2. COnsider context and subject matter knowledge
.
.
.

## Influential Points
>influential points : data points that have a disproportionately large impact on the regression model's coefficients and fitted values. 
> Detection: Cook's Distance: One common measure to identify influential points in Cook's distance. IF a point has a Cook's distance signficantly larger than others, it is condsidered influential.

```{r}
#install.packages("broom")
```

```{r}
library(broom)
library(dplyr)
dat_ins <- mod5 |> augment(ins)
```

```{r}
filter(dat_ins, .std.resid > 2)

```

> according to this data and this metric, all these points, 74 observations will not meet this criteria.

7/31/24 

```{r}
library(tidyverse)
library(broom)
```
> we will build a model w/ all the variables and then decide which ones we'll drop

```{r}
model1<-lm(charges~., data = insurance)
summary(model1)
```
> we use augment to get our predictions (.fitted), our actual charges, all our residuals (.resid), all of our leverages (.hat), and our cook distance (.cooksd), and one for all of our outliers (.std.resid)

```{r}
diagnostics <- model1 |> augment(data = insurance)
```

> we will filter our dataset to look at any outliers outside 3. we are going to get this our and have a dataset of all outliers
> for all observations that meet this restriction, we will call it an outlier

```{r}
outliers <- filter(diagnostics, abs(.std.resid) > 3)

```

> same thing for leverage (.hat). Let's create 

```{r}
leverage <- filter(diagnostics, .hat>2*(6 + 1)/nrow(insurance))
```

> same for influence (.cooksd), let's find our outliers 

```{r}
influence <- filter(diagnostics, .cooksd>4 / nrow(insurance))

#view(insurance)
```

> Now, we will address how our model will do when we have other things affecting it? within this dataset, our model is great, but with new values, how will this change

## Model Complexity, Overfitting/Underfitting

* Model complexity refers to the sophistication of flexibility of a model in capturing relationships between predictors and the response variable. In simple terms, it reflects how intricate the model is in representing the underlying patterns in the data

* Underfitting/Overfitting: model complexity is a crucial consideration in machine learning because overly simple models may underfit the data, failing to capture important patterns, while overly complex models may overfit the data, capturing noise instead of true underflying relationships. Achieving the right balance of complexity is essential for building modeles that generalize well to unseen data

Solutions:

1. Bootstrapping
2. Train-Test Split
3. Cross-Validation
4. Regularization

Model Performance Metrics:

There are several ways to measure the performance of a predictive model. For linear models, metrics include
* R^2 and Adjusted R^2
* Root Mean Squared Error (RMSE)
* Mean Absolute Error (MAE)

WHY ARE WE DOING THIS?
* If we have two different models (one with charges~. and one with charge~ (everything but sex)), we want the smallest RMSE. Let's write some code to do this in a simple case.

> Train Test Split Example

* Remember, this is without replacement!!!!

```{r}
#make sure to specify the split size
#80% of 1338 is not a descrete (whole) number. Make sure to use floor or ceiling functions
split <- sample(1:nrow(insurance), size = floor(0.8*nrow(insurance)))


train_data <-insurance[split,]
test_data <- insurance[-split, ]
```

> now that we have the test and train, we will build our model

```{r}
model2 <- lm(charges~age + bmi + children + smoker + region, data = train_data)

summary(model2)
```
> **NOW, let's see how good our model is**

```{r}
#(new data, test data)
predictions <- predict(model2, test_data)

#Root Mean Squared Error
rmse <- sqrt(mean((predictions - test_data$charges) ^ 2))

rmse
```

```{r}
#we could also just call the RMSE function
#RMSE(predictions, test_data$charges)

#^ there's an error, so be careful
```
> If we wanted to find the MAE, we could just take the absolute value of (ysubi - ysubi hat)
