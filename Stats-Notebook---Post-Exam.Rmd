---
title: "Stats Notebook - Post Exam"
author: "Courtney Hodge"
date: "2024-07-30"
output: html_document
---

7/30/24 Class

```{r}
insurance <- read.csv("C:\\Users\\hodge\\Desktop\\UVA_Coding_Folder\\Statistics-6021\\insurance.csv")
```

```{r}
#view(insurance)
```

```{r}
modl <- lm(charges~smoker, data=insurance)

summary(modl)
```

```{r}
t.test(charges~smoker,data= insurance)
```

```{r}
new <- data.frame(smoker="yes")
predict(modl, new)
```

```{r}
mod2 <- lm(charges~region, data = insurance)
summary(mod2)
```

```{r}
insurance$region <-factor(insurance$region, levels = c("southeast", "northeast", "northwest", "southwest"))
```

```{r}
insurance$region2 <-factor(insurance$region, levels = c("southeast", "northeast", "northwest", "southwest"))

mod3 <- lm(charges~region2, data = insurance)
summary(mod3)
```

```{r}
anov <- aov(charges~region, data = insurance)
```


> why are we doing all of this work? For the ANOVA test in this case, it is telling us that at the 5% level, we can go ahead and reject the null, meaning at least one of these means are different. What after? Tukey comparison to see which one is really different.

```{r}
TukeyHSD(anov)
```
> At least we can see which of theintervals are both negative. Southwest tells us that on average, southwest is lover than southeast.

>Now, we are going back to smoker, but we are using age. Let's make a boxplot before we start to build the model. Here, age is our predictor.

```{r}
library(ggplot2)
ggplot(insurance, aes(x = age, y = charges)) + geom_jitter()
```
> IF we fit the model above, do you think it will be agood model? The answer is no.

```{r}
library(ggplot2)
ggplot(insurance, aes(x = age, y = charges)) + geom_jitter() + geom_smooth(method = "lm", se = F)
```

> we can see that the line is actually trying to do the middle of smoker charges instead of non-smoker charges.

> we are going to color the dots based on the smoker status. Now, we can see what's really going on and this is why **data exploration is important**. All the outliers, the high charges, on average, cooresponds to smokers.

```{r}
library(ggplot2)
ggplot(insurance, aes(x = age, y = charges, color = smoker)) + geom_point()
```

```{r}
library(ggplot2)
ggplot(insurance, aes(x = age, y = charges, color = smoker)) + geom_jitter() + geom_smooth(method = "lm", aes(group = smoker))
```
> now, we have two lines. one that makes a predictor for smokers and one that makes a predictor for non-smokers. Before we build the model, we will have an indicator function that fits the model.

```{r}
mod4<- lm(charges~age + smoker, data = insurance)
summary(mod4)
```

> in terms of an equation, what does this model look like? So, in terms of non-smokers, for every you that you grow, you pay $274.87 more dollars in your insurance plan. For smokeryes, every time you move from being a smoker to non smoker, your health insurance moves to 23855.30 at age 0.

## Using Dummy Variables to capture Interactions Between Explanatory Variables

```{r}
mod5 <- lm(charges~age * smoker, data = insurance)
summary(mod5)
```
> Every time your age incrases by 1, your smokers will increase by 267.25 + 37.99.

```{r}
ggplot(insurance, aes(x = age, y = charges, color = smoker)) + geom_jitter() + geom_smooth(method = "lm", model.extract(mod5), se = F)
```

```{r}
new_dat <- data.frame(age=26, smoker = "no")
predict(mod5, new_dat,interval = "prediction")
```

> for a particular person, Disha, her insurance cost will be within $-7700.646 $17414.75 thousands of dollars.

## building a model with everything
```{r}
ins <- insurance[,-8]
```

```{r}
mod10 <- lm(charges~., data = ins)
summary(mod10)
```
> the residual standard error meausres how well we are going to do. This model will error in health care coverage by about $6,062 dolalrs. SSE (sum of squared error) from the population model. MSE = SSE/(n-p-1) is the residual standard error. 
> SSE/n

## Outliers

> leverage points, influential points, outliers
> how do you use your model to get rid of some of these variables if they are presenting a big change to our model

> Outlier: Outliers are data points with the largest residuals. Detection:

1. Scatterplot
2. Residual Plot
3. Standardized Residuals: Residuals divided by their estimated standard deviation.

> An outlier can be an x-outlier (when predictor variable has an unusual value) or a y-outlier (when residual variable has an unusual value). 

>Treating Outliers: 
1. Transform response
2. Consider context and subject matter knowledge
3. Measurement error>
4. Winsorize: Replace extreme valeus with the nearest values that are not outliers. For ex., replace valeues about the 95th percentile with the value at the 95th percentile
5. Trim/revmove point (last resort)
6. Use other techniques like quantile regression or Random Forest which are resistant to outliers.

> Leverage Points: dat apoints that hvae **extreme values** for a predictor variable. These points have a large influence on the fitted values of the model, but not necessarily on the regression coefficients themselves.
>Detection:
1. Leverage values are in the diagonals of the Hat matrix:(X^TX)^-1(X^T)
2. High leverage points are identified by leverage values greater than 2(p+1)/n, wher p is the number of predictors and 

>Treating High Leverage Points
1. Transform predicor
2. COnsider context and subject matter knowledge
.
.
.

## Influential Points
>influential points : data points that have a disproportionately large impact on the regression model's coefficients and fitted values. 
> Detection: Cook's Distance: One common measure to identify influential points in Cook's distance. IF a point has a Cook's distance signficantly larger than others, it is condsidered influential.

```{r}
#install.packages("broom")
```

```{r}
library(broom)
library(dplyr)
dat_ins <- mod5 |> augment(ins)
```

```{r}
filter(dat_ins, .std.resid > 2)

```

> according to this data and this metric, all these points, 74 observations will not meet this criteria.

7/31/24 

```{r}
library(tidyverse)
library(broom)
```
> we will build a model w/ all the variables and then decide which ones we'll drop

```{r}
model1<-lm(charges~., data = insurance)
summary(model1)
```
> we use augment to get our predictions (.fitted), our actual charges, all our residuals (.resid), all of our leverages (.hat), and our cook distance (.cooksd), and one for all of our outliers (.std.resid)

```{r}
diagnostics <- model1 |> augment(data = insurance)
```

> we will filter our dataset to look at any outliers outside 3. we are going to get this our and have a dataset of all outliers
> for all observations that meet this restriction, we will call it an outlier

```{r}
outliers <- filter(diagnostics, abs(.std.resid) > 3)

```

> same thing for leverage (.hat). Let's create 

```{r}
leverage <- filter(diagnostics, .hat>2*(6 + 1)/nrow(insurance))
```

> same for influence (.cooksd), let's find our outliers 

```{r}
influence <- filter(diagnostics, .cooksd>4 / nrow(insurance))

#view(insurance)
```

> Now, we will address how our model will do when we have other things affecting it? within this dataset, our model is great, but with new values, how will this change

## Model Complexity, Overfitting/Underfitting

* Model complexity refers to the sophistication of flexibility of a model in capturing relationships between predictors and the response variable. In simple terms, it reflects how intricate the model is in representing the underlying patterns in the data

* Underfitting/Overfitting: model complexity is a crucial consideration in machine learning because overly simple models may underfit the data, failing to capture important patterns, while overly complex models may overfit the data, capturing noise instead of true underflying relationships. Achieving the right balance of complexity is essential for building modeles that generalize well to unseen data

Solutions:

1. Bootstrapping
2. Train-Test Split
3. Cross-Validation
4. Regularization

Model Performance Metrics:

There are several ways to measure the performance of a predictive model. For linear models, metrics include
* R^2 and Adjusted R^2
* Root Mean Squared Error (RMSE)
* Mean Absolute Error (MAE)

WHY ARE WE DOING THIS?
* If we have two different models (one with charges~. and one with charge~ (everything but sex)), we want the smallest RMSE. Let's write some code to do this in a simple case.

> Train Test Split Example

* Remember, this is without replacement!!!!

```{r}
#make sure to specify the split size
#80% of 1338 is not a descrete (whole) number. Make sure to use floor or ceiling functions
split <- sample(1:nrow(insurance), size = floor(0.8*nrow(insurance)))


train_data <-insurance[split,]
test_data <- insurance[-split, ]
```

> now that we have the test and train, we will build our model

```{r}
model2 <- lm(charges~age + bmi + children + smoker + region, data = train_data)

summary(model2)
```
> **NOW, let's see how good our model is**

```{r}
#(new data, test data)
predictions <- predict(model2, test_data)

#Root Mean Squared Error
rmse <- sqrt(mean((predictions - test_data$charges) ^ 2))

rmse
```

```{r}
#we could also just call the RMSE function
#RMSE(predictions, test_data$charges)

#^ there's an error, so be careful
```
> If we wanted to find the MAE, we could just take the absolute value of (ysubi - ysubi hat)

## Cross Validation
There are several cross validation techniques:

* K-fold CV
* Repeated K-fold CV
* Leave one out CV
* Hold-out CV, etc.


```{r}
#map_dbl(1:5, ~.x^2)
```

> how do we create our folds? use the caret library

```{r}
#library(caret)
```



```{r}
#CV <- function(data, k){
 # folds <- createFolds(insurance$charges, k = k)
  #map_dbl(folds, function(indices){
   # train_data <- data[-indices, ]
    #test_data <- data[indices, ]
    
  #  model5 <-lm(charges~.,-sex, data = train_data)
   # predictions <- predict(model5, test_data)
    #sqrt(mean((predicitons-test_Data$charges) ^ 2))
  #}) |> means()
#}
```

```{r}
#CV(insurance, 5)
```
```{r}
repeated <- replicate(10, CV(insurance, 5))
mean(repeated)
```

```{r}
model0 <- lm(charges~.-sex, data = insurance)
summary(model0)
```
8/1/24

```{r}
#library(caret)
#control <-trainControl
```

```{r}

model1 <- train(charges~.-sex, method = "lm", data = insurance)


```

> the train function can do everything lm can do and more. In this argument, you can add something that includes cross-validation too.

```{r}
control <- trainControl(method = "cv", number = 5)

model <- trainControl(method = "repeatedcv", number = 5, repeated = 10)
```

> by default it does a k-fold. make a 5 fold using number attribute

> by using "repeatedcv", you'll have to tell it the number of folds and number of repeats. So above, we are doing the 5-cv fold 10 times

```{r}
model2 <- train(charges~.-sex, method = "lm", trControl = control, data = insurance)
summary(model2)
```
> above, the model will be the same, but it does the cross validation for us.

```{r}
#library(Rtools)
```
```{r}
model2$results$RMSE
```

> this should produce the same number as yesterday

```{r}
#"C:\rtools44"
writeLines('PATH="${C:\rtools44};${PATH}"', con = "~/.Renviron")
Sys.which("make")

```
### Regularization
Shrinkage: Ridge and Lasso

* We typically cannot achieve both low Bias and low viarance simultaneously(which leads to bias-variance tradeoff)

* Shrinkage methods adds an amount of smart bias to reduce variance. Hence enhancing intepretations of the bias version of the estimator.

* The term ”shrinkage” typically refers to methods that pull estimates
towards a central value, often zero. This can help improve the
model’s performance on new data by preventing overfitting.

Ridge and Lasso are Shrinkage techniques

```{r}
laliga <- read.csv("C:\\Users\\hodge\\Desktop\\UVA_Coding_Folder\\Statistics-6021\\laliga.csv")
```

```{r}
laliga2 <-laliga[,-c(1,3,9)]
```

```{r}
modellaliga <-lm(Points~., data = laliga2)
summary(modellaliga)
```
> from the summary above, we can see that all these predicotrs are useless

> let's try another approach

> step aic got rid of some variables, but it still did not help.

> next, we are going to look at the VIF of the model to look at multicolinearity

```{r}
aic <- MASS::stepAIC(modellaliga, direction = "both", Trace = F)
summary(aic)
```

```{r}
#car::vif(aic$model)
```

### Glmnet

> Loading the glmnet to do all the mathematical computations for us. before we use it, make sure we don't have any NAs
> We have to find the design matrix
> the 0 after the ~ gets rid of all the 1s in the identity matrix, which is what we want

```{r}
library(glmnet)
design_matrix <- model.matrix(Points~0+., data = laliga2)
#View(design_matrix)

response_var <- laliga2$Points

rmodel <- glmnet(x = design_matrix, y = response_var, alpha = 0) #specifies that we are doing Ridge Regression!!!!!

#if you want to do a glmnet, then you would say alpha is 0.5 so that you can use a little of Ridge and Lasso

plot(rmodel, label = T, xvar = "lambda")
```
> In the graph above, we should get rid of all the variables taht aren't close to zero

```{r}
plot(rmodel, lable = T, xvar = "dev")
```
```{r}
length(rmodel$lambda)
```

```{r}
coef(rmodel)[,50]
```

> the question is, which is the best lamda out there?
> We can answer this my doing cross validation again

```{r}
kcvglmnet <- cv.glmnet(x = design_matrix, y = response_var, alpha = 0, nfolds = 3) #typically, you want to do more than 2
```

```{r}
kcvglmnet$lambda.min
```

> you can use the minimum, but people usually find the lambda that is 1sd away. This is the principal of passer money

> Anywys, now lets make a prediction on the model using the lamda above

```{r}
predict(rmodel, type = "response", s = kcvglmnet$lambda.min, newx = design_matrix[1:2, ]) 
#predicts the first two rows of the dataset
#s is the optimal value for lambda
```
> the numbers above predicts that the points for barcelona was 77 and 74, which is MUCH BETTER than the original linear model

```{r}
#what about using lasso?
kcvglmnet <- cv.glmnet(x = design_matrix, y = response_var, alpha = 1, nfolds = 3) #typically, you want to do more than 2

kcvglmnet$lambda.min

predict(rmodel, type = "response", s = kcvglmnet$lambda.min, newx = design_matrix[1:2, ]) 

```

8/2/24
```{r}
laliga2 <- laliga[,-c(1,3,9)]

X <- model.matrix(Points~0+., data = laliga2)
y <- laliga2$Points

rmodel <-glmnet(x= X, y=y, alpha =0)
kcvglmnet <- cv.glmnet(x=X, y=y, alpha = 0, nfolds = 3)
kcvglmnet$lambda.min

kcvglmnet$lambda.1se
```
> above, we are just finding the minimum lambda after a 3-cv fold, and we are finding the lambda 1 standard deviation away using the lambda.1se

```{r}
predict(rmodel, type = "coefficient", s = kcvglmnet$lambda.1se, newx = X[1:2,])
```
> if we wanted to add a verticle line to the log lambda plot, we can do the following. It shows us the optimal lambda to pick based on the 1 standard devation rule. The numbers next to each line are the columns from the dataset we want to look out. So column 7 and column 6 are those that values that you pick.

```{r}
plot(rmodel, lable = T, xvar = "lambda") + abline(v = log(kcvglmnet$lambda.1se))
```

> new stuff

# Principal Component Analysis (PCA) and Regression
>more rows in the dataset is good, but more columns is bad
>PCA is another techinique to shrink the number of predictors in our model
>We'll also talk about regression using PCA

PCA is a multivariate technique designed to summarize the most important features and relations of *p* numerical random variables

* PCA computes a new set of variables, the principal component that contains the same information as the p numerical random variables but expressed in a more convenient way

* the goal of PCA is to retain only a limited number of principal components that explain most of the information, therefore performing dimension reduction

3. remarkable, PCA computes the principal components in an ordered way: the first principal component explains the most of the information (quantified as the variance) of *p* numerical random variables, and then the explained information decreases monotonically down to the last principal component. That is: 

```{r}
pca <- princomp(laliga2, fix_sign = T)
summary(pca)
```
> because we have 17 variables, we have 17 components. The first pricnipal component captures 77% of of the data. The second, 16%. as you go down, the variability gets smaller. It looks like the first 3 captures all the variability of the data compared to the other principal component.

> let's plot it out

```{r}
plot(pca, type = "l")
```
> this plot van visually tell you the information in the pca's summary

```{r}
pca2 <- princomp(laliga2, cor = T, fix_sign = T) #standardizes everything

pca2$loadings
```
> the loadings are the numbers in front of each linear compination of all the principal components.

$\Gamma = $

```{r}
pca2$scores
```


> the pca$score is a way to bring down the actual observations in the data columns to represent it in terms of pca. So it's the entire dataset, and the values are being transformed to the principal components

> let's plot to see what the heck is happening. You can do a biplot() to show you some cool info about how some of these variables are pulling towards each of them

```{r}
biplot(pca2)
```


> based on the biplot, the numbers inside the plot are the row numbers. the magnitude of the arrows tells you how strongly correlated the variables are.

> now, we want to separate the predictors from PCA in a design matrix and then bring that into the model

```{r}
laliga3 <- laliga2[, -1]
```

> we are going to run a PCA on laliga3

```{r}
pca3 <- princomp(laliga3, cor = T, fix_sign = T)
summary(pca3)
```
> we will then pull the points from this data frame to build a linear model

```{r}
pca_data <- data.frame(Points = laliga$Points, pca3$scores) #build our dataframe

pcareg <- lm(Points~., data = pca_data)#build our regression model

summary(pcareg)
```

> we now have all the residuals in terms of the components. All the p-vals are significant except for the 16th principal component, hence the lack of asterisks.

> let's look at the VIF to determine if this model is good or not

```{r}
car::vif(pcareg)
```

```{r}
pcareg2 <- lm(Points~Comp.1 +Comp.2+ Comp.3, data = pca_data )
summary(pcareg2)
```
```{r}
car::vif(pcareg2)
```

> if we bring it down to the first 3 components that capture the variablility of the model, we don't have multicolinearity and all the VIFs are 1. If all the VIFs are 1, that means our tollerance is also 1. This makes sure that these matrixes are orthagonal. 

> now, supose we use the first three components. How do we make predictions? We know that the original formula is in terms of gamma, but we have to trace it back to the original variables, x, which is more intricate. How do I avoide this step b/c it's so intricate

> there's a package in r to do this.

```{r}
library(pls)
```

```{r}
laliga4 <- subset(laliga2, select = -c(Wins, Draws, Loses))
pcareg3 <-pcr(Points~., data = laliga4, scale = T, ) #does principal component regression

summary(pcareg3
        )
```

> so, our first component is accounting for 84.99 of our variability, and with 13 components, we found that that num of components accomplishes 98.74 of our variability

> so, say that we want to predict the first two rows of laliga4. We can use the predict function here to do that. PCR doesn't have the same function like stepaic that we used in the linear model. It has its own library, so we must use what's in the package

```{r}
new_dat = laliga4[1:2, ]
predict(pcareg3, new_dat, ncomp = 13)
```
> you don't need to pick all the principal components.

> how do you decide how many components to pick

# Logistic Regression
> used to model the probability of a dichotomous outcome variable. For instance win/loss in an NBA game based on data on free throw percentage, number of rebounds, steals, blocks, etc
> Could work with political candidate winning/losing an election
> probability of approval/non-approval of loan for a bank customer based on credit score

Interpreting the Slope Coefficients
> the change in the odds of the response for every one unit increase in the predictor, holing other predictors fixed

8/5/24

```{r}
NBA <- read.csv("C:\\Users\\hodge\\Downloads\\nba.csv")
```

```{r}
library(tidyverse)
```
> create home_away table, recode w.l
> searches the matchup column to look for "vs."

```{r}
nba2 <- NBA |> 
  mutate(home_away = ifelse(grepl("vs", MATCHUP), "home", "away"),
         win = ifelse(W.L == "W", 1, 0)) |> 
  rename( FG. = FGP,  X3P. = TPP, FT. = FTP) |> 
  select(W.L, win, home_away, MIN, PTS, FG.,  X3P., FT., REB, AST, STL, BLK, TOV, PF)
View(nba2)
```

> #1 is the home game advantage real?

```{r}
ggplot(nba2, aes(x = home_away, fill = factor(win), color = factor(win ))) +
  geom_bar(position = "fill")
```
> Yes, it looks like a greater percentage of home games results in a win. we will need to do a chi-square test of independence to conclude that baout all NBA games

> #2 visually checking for multicollinearity with correlation plot for all the numeric variables 

> remove win and home_away before correlation matrix (because not numeric)

```{r}
dat <- nba2[, -c(1,2,3)]
cor_mat <- round(cor(dat), 2)

```

# make corplot
```{r}
ggcorrplot::ggcorrplot(cor_mat, lab = T, method = "circle", type = "lower")
```
# there might be colinearity between PTS and FG.

```{r}
logit_model1 <- glm(win~.-W.L, nba2, family = "binomial")
summary(logit_model1)
```
> use the train function from the caret package (still can't use it big sigh)

this is just the SAME way to do the above

```{r}
#library(caret)
# logit_model2 <- train(W.L~.-win, nba2, method = 'glm', family = "binomial")
#summary(logit_model2)
```

> Selecting useful predictor for predicting the outcome of the game

hypothesis test
* idea 1, revised model after removing PTS:
```{r}
logit_model3 <- glm(win~.-W.L-PTS, nba2, family = "binomial")
summary(logit_model3)
```
Look at the AIC here. The lower the AIC value, the better. This helps make sure our model is good.

> OR

```{r}
install.packages(MLR)
#library(MLR)libra"mlr"ry(MLR)
#need the caret library for this
logit_model4 <- train(W.L~.-win-PTS, nba2, method = "glm", family = "binomial")
summary(logit_model4)
```

```{r}
aic <- MASS::stepAIC(logit_model1, direction = "both", trace = FALSE)
summary(aic)
```


